{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15. Keras_Tuner",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MWFK/TensorFlow-Certificate-study-materials/blob/main/15.%20Keras_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqEvyTemtnv"
      },
      "source": [
        "# Getting started with KerasTuner\n",
        "\n",
        "**Authors:** Luca Invernizzi, James Long, Francois Chollet, Tom O'Malley, Haifeng Jin<br>\n",
        "**Date created:** 2019/05/31<br>\n",
        "**Last modified:** 2021/06/07<br>\n",
        "**Description:** The basics of using KerasTuner to tune model hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvPuH_bjmtn1"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bn0zvBhmtn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52bc4e5c-29ad-4be1-a077-59e3c9600e81"
      },
      "source": [
        "!pip install keras-tuner -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 30.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 35.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.9 MB/s \n",
            "\u001b[?25h  Building wheel for kt-legacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewx94_bNmtn4"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Here's how to perform hyperparameter tuning for a single-layer dense neural\n",
        "network using random search.\n",
        "First, we need to prepare the dataset -- let's use MNIST dataset as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBHtp5qOmtn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e901d5cc-a7db-4c9f-8cc7-c4a7dbfbbb9c"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x[:-10000]\n",
        "x_val = x[-10000:]\n",
        "y_train = y[:-10000]\n",
        "y_val = y[-10000:]\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2a077N-mtn5"
      },
      "source": [
        "## Prepare a model-building function\n",
        "\n",
        "Then, we define a model-building function. It takes an argument `hp` from\n",
        "which you can sample hyperparameters, such as\n",
        "`hp.Int('units', min_value=32, max_value=512, step=32)`\n",
        "(an integer from a certain range).\n",
        "\n",
        "This function returns a compiled model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzKYN5_pmtn6"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Look for the best number of units\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # Look for the best learning_rate\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ItdbKjtmtn7"
      },
      "source": [
        "## Start the search\n",
        "\n",
        "Next, let's instantiate a tuner. You should specify the model-building function, the\n",
        "name of the objective to optimize (whether to minimize or maximize is\n",
        "automatically inferred for built-in metrics), the total number of trials\n",
        "(`max_trials`) to test, and the number of models that should be built and fit\n",
        "for each trial (`executions_per_trial`).\n",
        "\n",
        "We use the `overwrite` argument to control whether to overwrite the previous\n",
        "results in the same directory or resume the previous search instead.  Here we\n",
        "set `overwrite=True` to start a new search and ignore any previous results.\n",
        "\n",
        "Available tuners are `RandomSearch`, `BayesianOptimization` and `Hyperband`.\n",
        "\n",
        "**Note:** the purpose of having multiple executions per trial is to reduce\n",
        "results variance and therefore be able to more accurately assess the\n",
        "performance of a model. If you want to get results faster, you could set\n",
        "`executions_per_trial=1` (single round of training for each model\n",
        "configuration)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNdjR4I9mtn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ea52f7-bd06-4a15-9b4a-81ee1a4d4751"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JWeRWHNmtn-"
      },
      "source": [
        "Then, start the search for the best hyperparameter configuration.\n",
        "The call to `search` has the same signature as `model.fit()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMf_ZNHhmtn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901a92ab-648c-48cb-dc68-e3dbe6fd0148"
      },
      "source": [
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 09s]\n",
            "val_accuracy: 0.9608999788761139\n",
            "\n",
            "Best val_accuracy So Far: 0.9733999967575073\n",
            "Total elapsed time: 00h 01m 24s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe_Qah3smtn-"
      },
      "source": [
        "Here's what happens in `search`: models are built iteratively by calling the\n",
        "model-building function, which populates the hyperparameter space (search\n",
        "space) tracked by the `hp` object. The tuner progressively explores the space,\n",
        "recording metrics for each configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq8Pacwfmtn-"
      },
      "source": [
        "## Query the results\n",
        "\n",
        "When search is over, you can retrieve the best model(s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaR9vvNamtn-"
      },
      "source": [
        "models = tuner.get_best_models(num_models=2)\n",
        "tuner.results_summary()\n",
        "# You will also find detailed logs, checkpoints, etc, in the folder my_dir/helloworld, i.e. directory/project_name."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Kn3xFtp9Hc"
      },
      "source": [
        "##############################################################################################################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmPN-0E9mtoD"
      },
      "source": [
        "## The search space may contain conditional hyperparameters\n",
        "\n",
        "Below, we have a `for` loop creating a tunable number of layers,\n",
        "which themselves involve a tunable `units` parameter.\n",
        "\n",
        "This can be pushed to any level of parameter interdependency, including recursion.\n",
        "\n",
        "Note that all parameter names should be unique (here, in the loop over `i`,\n",
        "we name the inner parameters `'units_' + str(i)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOXSnw3nmtoE"
      },
      "source": [
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Look for the best number of layers, and in each layer, look for the best number of units =)\n",
        "    for i in range(hp.Int(\"num_layers\", 2, 20)):\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    \n",
        "    # Look for the best learning rate\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI3zcrNpmtoE"
      },
      "source": [
        "## You can use a HyperModel subclass instead of a model-building function\n",
        "\n",
        "This makes it easy to share and reuse hypermodels.\n",
        "\n",
        "A `HyperModel` subclass only needs to implement a `build(self, hp)` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd-h9GH5mtoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9909f287-4fec-4916-84fd-3d0d0837f4ed"
      },
      "source": [
        "from keras_tuner import HyperModel\n",
        "\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = keras.Sequential()\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(\n",
        "            layers.Dense(\n",
        "                units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.Dense(self.classes, activation=\"softmax\"))\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(\n",
        "                hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "            ),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "        return model\n",
        "\n",
        "\n",
        "hypermodel = MyHyperModel(classes=10)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 10s]\n",
            "val_accuracy: 0.9659000039100647\n",
            "\n",
            "Best val_accuracy So Far: 0.9659000039100647\n",
            "Total elapsed time: 00h 00m 26s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcWgGpPCmtoF"
      },
      "source": [
        "## KerasTuner includes pre-made tunable applications: HyperResNet and HyperXception\n",
        "\n",
        "These are ready-to-use hypermodels for computer vision.\n",
        "\n",
        "They come pre-compiled with `loss=\"categorical_crossentropy\"` and `metrics=[\"accuracy\"]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnysTU60mtoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5656a7e-307e-47d1-9776-f1814227cf14"
      },
      "source": [
        "from keras_tuner.applications import HyperResNet\n",
        "\n",
        "hypermodel = HyperResNet(input_shape=(28, 28, 1), classes=10)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 16s]\n",
            "val_accuracy: 0.10000000149011612\n",
            "\n",
            "Best val_accuracy So Far: 0.10999999940395355\n",
            "Total elapsed time: 00h 01m 13s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hb2JuN5mtoG"
      },
      "source": [
        "## You can easily restrict the search space to just a few parameters\n",
        "\n",
        "If you have an existing hypermodel, and you want to search over only a few parameters\n",
        "(such as the learning rate), you can do so by passing a `hyperparameters` argument\n",
        "to the tuner constructor, as well as `tune_new_entries=False` to specify that parameters\n",
        "that you didn't list in `hyperparameters` should not be tuned. For these parameters, the default\n",
        "value gets used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZTa3JLdmtoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0667c0cb-1d70-4f2d-a963-334901fad434"
      },
      "source": [
        "from keras_tuner import HyperParameters\n",
        "from keras_tuner.applications import HyperXception\n",
        "\n",
        "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
        "\n",
        "hp = HyperParameters()\n",
        "\n",
        "# This will override the `learning_rate` parameter with your\n",
        "# own selection of choices\n",
        "hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    hyperparameters=hp,\n",
        "    # `tune_new_entries=False` prevents unlisted parameters from being tuned\n",
        "    tune_new_entries=False,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 05s]\n",
            "val_accuracy: 0.09000000357627869\n",
            "\n",
            "Best val_accuracy So Far: 0.10999999940395355\n",
            "Total elapsed time: 00h 00m 20s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63Ff6KiLmtoH"
      },
      "source": [
        "## About parameter default values\n",
        "\n",
        "Whenever you register a hyperparameter inside a model-building function or the `build` method of a hypermodel,\n",
        "you can specify a default value:\n",
        "\n",
        "```python\n",
        "hp.Int(\"units\", min_value=32, max_value=512, step=32, default=128)\n",
        "```\n",
        "\n",
        "If you don't, hyperparameters always have a default default (for `Int`, it is equal to `min_value`).\n",
        "\n",
        "## Fixing values in a hypermodel\n",
        "\n",
        "What if you want to do the reverse -- tune all available parameters in a hypermodel, **except** one (the learning rate)?\n",
        "\n",
        "Pass a `hyperparameters` argument with a `Fixed` entry (or any number of `Fixed` entries), and specify `tune_new_entries=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onv9onbImtoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e1435a-53d2-4d45-d75b-0a70b2354e43"
      },
      "source": [
        "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
        "\n",
        "hp = HyperParameters()\n",
        "hp.Fixed(\"learning_rate\", value=1e-4)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    hyperparameters=hp,\n",
        "    tune_new_entries=True,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 19s]\n",
            "val_accuracy: 0.11999999731779099\n",
            "\n",
            "Best val_accuracy So Far: 0.11999999731779099\n",
            "Total elapsed time: 00h 00m 37s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaBU2Bk1mtoH"
      },
      "source": [
        "## Overriding compilation arguments\n",
        "\n",
        "If you have a hypermodel for which you want to change the existing optimizer,\n",
        "loss, or metrics, you can do so by passing these arguments\n",
        "to the tuner constructor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn_WjkcXmtoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b98f06d-6113-4fc1-de87-fde15bbdbd39"
      },
      "source": [
        "hypermodel = HyperXception(input_shape=(28, 28, 1), classes=10)\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    optimizer=keras.optimizers.Adam(1e-3),\n",
        "    loss=\"mse\",\n",
        "    metrics=[\n",
        "        keras.metrics.Precision(name=\"precision\"),\n",
        "        keras.metrics.Recall(name=\"recall\"),\n",
        "    ],\n",
        "    objective=\"val_loss\",\n",
        "    max_trials=3,\n",
        "    overwrite=True,\n",
        "    directory=\"my_dir\",\n",
        "    project_name=\"helloworld\",\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train[:100], y_train[:100], epochs=1, validation_data=(x_val[:100], y_val[:100])\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 18s]\n",
            "val_loss: 0.08999796956777573\n",
            "\n",
            "Best val_loss So Far: 0.08999796956777573\n",
            "Total elapsed time: 00h 00m 50s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}